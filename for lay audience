#Python library
all the code that is available if you have Python installed on your machine. If a module is part of the standard library, you are sure that your Python program can still use it: there is nothing to install.
While The Python Language Reference describes the exact syntax and semantics of the Python language, this library reference manual describes the standard library that is distributed with Python. It also describes some of the optional components that are commonly included in Python distributions. Python’s standard library is very extensive, offering a wide range of facilities as indicated by the long table of contents listed below

#GitHub + Version control
Git Hub is a platform for coders to collaborate when updating codes. It is based on two functions; Git and Version control. Version control enables someone when they wants to work with a data base to develop it, instead of taking the official code source this person can branch and merge data. Branching enables to duplicate a data base to modify it surely and efficiently. Merging is once the developer makes his part of the code work properly, he can merge that code into the main source code to make it official. Git is a specific open source version control system, it allows codebase and their history on every developer computer.

#Sentiment analysis
Analyzing sentiments on tweets means that we use a code to understand the tone of the tweet, which can rather be positive or negative. The code is able to define the feeling expressed in the message thanks to an analysis of the words and the syntax used. It is the use of 'natural language processing, textual analysis and computational linguistics to identify and extract subjective information from different sources'.

#Topic modelling
Topic Modelling is a process for automatically identifying the subjects present in a text object and deriving hidden patterns exposed by a corpus of text. Thus, help for a better decision making.
As there are more and more data available and this one is not structured enough, topic modeling has been invented to enable people to find the information needed more efficiently.

#Cleaning tweets
Once extracted, the tweets to be used can come with unwanted html characters, bad grammar and poor spellings. Cleaning tweets enable to make them understandable; it decodes the data by correcting the spelling, remove all the stop-words, the punctuation, then it standardize the words, the last step is removing of the URL. Once these steps are done, the tweets are readable and ready to be used in a further analysis.

#Hydrating tweets
Hydrator is an electron-based desktop application for hydration of Twitter credentials. Twitter's terms of use do not allow distribution of the entire JSON for tweets datasets to third parties. However, they do allow the sharing of tweets' identifying data. Hydrator helps you to convert these tweet IDs back to JSON and also to CSV from the comfort of your office for later use.

#Magnitude
The magnitude assigned to a tweet is a number, assigned to something found through analysis, so that it can be compared to others numerically. It bases on standards.

#Pearson’s correlation coefficient
Pearson’s correlation coefficient, is a statistic that measures linear correlation between two variables X and Y, it permits to establish if there is a relationship between the two variables used.
The statistical measure is between -1 and 1. The closer the number is to zero, the more the correlation is non-existent. 

#Hexbin visualisation 
Hexbin diagrams take lists of X and Y values and return what looks somewhat like a point cloud, but where the entire graphic space has been divided into hexagons (like a honeycomb) and all the points have been grouped into their respective hexagonal regions with a color gradient indicating the density of each hexagonal area. It is used to visualize density, where the hexagon as a shape makes it easy to create contiguous areas while dividing the entire space into discrete units.


