### Gathering the twitter data: 
At first we planned to gather tweets ourselves using the api, but realised that we would miss a lot of covid-related tweets as our self-made covid dictionary is not extensive and would inevitably miss covid-related tweets. Also we cannot retrieve historical tweets, only ones that are posted while the API is receiving requests
Decide to go with a dataset on github to focus our time on the analysis

### Data visualisation with plotly:
Making a slider for one of the chloropleth maps was one of the more challenging parts, especially learning all the optional arguments that could be used in function calls to customize the plot
learning curve but used documentation extensively, as well as example code and forums such as Stack Overflow

### Running the Google Natural Language API for sentiment analysis:
Most accurate model but is not free: were able to take advantage of it by setting up several free  trial accounts but that means that it is less easily reproducible
Some issues when using the API as a break in internet connection or interrupting the running cell meant work was lost, which was an issue because we all were on trial accounts and had limited credit to use
Also if the cell was run twice all variables were overwritten so the process had to be redone > solved by adding a line at the end of the cell which saved the temporary data frame in case the cell was rerun

### Internet connection issues
A lot of the computational work required a stable and suitable internet connection, especially when doing sentiment analysis and hydrating the tweets
Not all of us had adequate speed so work had to be redistributed to account for that, with people with better internet doing more of the hydration/sentiment analysis

### Backup data:
Have a shared drive online for us to put datasets that have been preprocessed and cleaned
One team member did not upload all their files online and when they tried to install software packages for sentiment analysis, lost several days worth of hydrated tweets that were permanently deleted, which slowed down the process
Learned how important it is to backup as soon as we have new data

### Google colab runtime:
Runtime disconnects after a few hours: this means work is lost if it takes longer than that and is not saved in the meantime
Output files are also permanently deleted unless downloaded
Transitioned to visual studio code which is a software that provides an interface for coding similar to colab and supports jupyter notebooks, but does not require internet connection as it runs locally
It also saves files directly to the computer and has more features such as linting (automatically flags syntax mistakes), and integration with github
